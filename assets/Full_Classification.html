<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Merged Document</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Peeping Inside Black box : Classification</h1>
    <section>
     <h2>Classification with Neural Network</h2>
    <p>Machine Learning Classification approach is based on creating a plane or probability distribution approach. This includes SVM and Logistic Regression, which are based on separation plane approach and naive base is based on probability distribution approach.</p>
    <p>We will explore what could be the approach taken by Deep learning network. Can it be something other than these approaches? Let's start with a simpler problem with two blobs in a 2-D plane (<a href="#fig:blob">Fig. 1</a>). This would pave us the path to approach for complex problem and problems in higher dimensions.</p>
    <figure id="fig:blob">
    	<img src="blob.png" alt="Two blobs of Class 0 and Class 1 in 2-D plane." style="width: 350px; height: 300px;">
        <figcaption>Fig. 1: Two blobs of Class 0 and Class 1 in 2-D plane.</figcaption>
    </figure>
    
    <p>The figure presents a dataset which is described by coordinate \(x\) and \(y\), where the first blob is under the class 0 and another class 1. We can easily make out of it, that \(y=x\) <a href="#fig:blob">Fig. 1</a>, separate this data in equal proportion. How do we understand this quantitatively?</p>
    <p>We create a neural network which accepts two input \(x\) and \(y\). Let's start with neurons in a basic architecture:</p>
    <div>
        \[z = ax + by + c\]
    </div>
    <h2>Separating Line (y=x):</h2>
    <ul>
        <li>The line \(y = x\) serves as a visual boundary between the two clusters.</li>
        <li>Points above the line generally belong to one cluster, while points below belong to the other.</li>
    </ul>
    <h2>Plane z = x - y: Fig. 1</h2>
    <ul>
        <li>This plane extends the concept of separation into three-dimensional space.</li>
        <li>It's defined by the equation \(z = x - y\), meaning it calculates the difference between the \(x\) and \(y\) coordinates of each point.</li>
    </ul>
    <h2>Class Separation:</h2>
    <ul>
        <li>Points with \(z < 0\) (where \(x - y < 0\)) are classified as belonging to Class 0.</li>
        <li>Points with \(z > 0\) (where \(x - y > 0\)) are classified as belonging to Class 1.</li>
    </ul>
    <p>For \(a=1\), \(b=-1\) and \(c=0\), we get:</p>
    \[z = x - y\]
    <p>The corresponding equation gives plot with z contour.</p>
    <figure>
        <img src="counout_sigmoid_blob.png" alt="(a) Contour plot of function \(z\). (b) Contour plot of function \(\text{sigmoid}(z)\).">
        <figcaption>(a) Contour plot of function \(z\). (b) Contour plot of function \(\text{sigmoid}(z)\).</figcaption>
    </figure>
    <p>After applying Sigmoid function:</p>
    \[\sigma(z) = \text{sigmoid}(z)\]
    <h2>Binomial Distribution:</h2>
    <p>The sigmoid function transforms the linear decision boundary \(z = x - y\) into a form resembling a probability distribution, specifically a binomial distribution with two possible outcomes (0 or 1).</p>
    <h2>Classification using Sigmoid: Fig. 1(b)</h2>
    <ul>
        <li>For \(\text{Sigmoid}(z) < 0.5\): Points with \(z < 0\) are classified as Class 0.</li>
        <li>For \(\text{Sigmoid}(z) > 0.5\): Points with \(z > 0\) are classified as Class 1.</li>
    </ul>
    <h3>Key Concepts</h3>
    <ul>
        <li><strong>Linear Separation:</strong> Straight line (or plane) effectively divides data.</li>
        <li><strong>Decision Boundary:</strong> Surface or threshold determining class membership.</li>
        <li><strong>Normalization:</strong> Transforming data to a common scale for analysis.</li>
        <li><strong>Binomial Distribution:</strong> Probability distribution with two possible outcomes.</li>
        <li><strong>Sigmoid Function:</strong> Used for classification and logistic regression.</li>
    </ul>
    <h3>Summary</h3>
    <ul>
        <li>The image demonstrates linear separation of data clusters using a contour plane.</li>
        <li>The sigmoid function introduces a probabilistic approach, mapping data to class probabilities.</li>
        <li>This combination of techniques is fundamental in various machine learning classification algorithms.</li>
    </ul>
    <h2>Image Analysis: Decision Boundaries and Softmax Function</h2>
    <p>This problem we dealt with the sigmoid function. However, we can also use softmax function. How do we approach for it. Two outputs are required, which is compared and provide, the probability for the class. A simple architecture is presented in Fig. 2.</p>
    <figure>
        <img src="2_D_Softmax.png" alt="Simple Architecture for Classification Problem.">
        <figcaption>Simple Architecture for Classification Problem.</figcaption>
    </figure>
    <p>Let's start with neurons in a basic architecture:</p>
    \[z_1 = a_1x + b_1y + c_1\]
    \[z_2 = a_2x + b_2y + c_2\]
    <p>for \(a_1=1\), \(a_2 = -1\), \(b_1=-1\), \(b_2=1\) and \(c_1=0\), \(c_2=0\)</p>
    <ul>
        <li><strong>Two Blobs:</strong> Distinct clusters of data points in 2D space. (Include image of two blobs)</li>
        <li><strong>Separation Line (y=x):</strong> Visually divides the blobs, suggesting a boundary.</li>
        <li><strong>Contour Planes (z1=x-y, z2=x-y):</strong> Define two surfaces that effectively partition the blobs in different ways.</li>
        <li><strong>Softmax Function:</strong> Applied to both \(z1\) and \(z2\) to create normalized probability distributions, aiding in class assignment. (Include image of softmax function)</li>
    </ul>
    <p>The \(z_1\) and \(z_2\) are the outputs of the model, We further apply softmax function over this output, this would give the probability distribution of both the classes. The Softmax function is defined as:</p>
    <figure>
        <img src="counout_softmax_blob.png" alt="2-D Classification with softmax. (a) The first plot defines the contour \(z_{1}\) with softmax function \(\phi(z_{1})\). (b) The second plot defines the contour \(z_{2}\) with softmax function \(\phi(z_{2})\).">
        <figcaption>2-D Classification with softmax. (a) The first plot defines the contour \(z_{1}\) with softmax function \(\phi(z_{1})\). (b) The second plot defines the contour \(z_{2}\) with softmax function \(\phi(z_{2})\).</figcaption>
    </figure>
    <p>The softmax function for a vector \( \mathbf{z} = (z_1, z_2, \dots, z_n) \) is given by:</p>
    \[\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}\]
    <p>For our problem the equation would transform as:</p>
    \[\phi(z_1) = \frac{e^{z_1}}{\sum_{j=1}^{2} e^{z_j}}\]
    \[\phi(z_2) = \frac{e^{z_2}}{\sum_{j=1}^{2} e^{z_j}}\]
    <p>The Equations defines the Fig. 3 as</p>
    <h3>Plot (a): Contour \(\phi(z_1)\)</h3>
    <ul>
        <li><strong>Class 0:</strong> Data points with \(z_1 > 0\), falling above the plane.</li>
        <li><strong>Class 1:</strong> Data points with \(z_1 < 0\), positioned below the plane.</li>
        <li><strong>Probabilities:</strong>
            <ul>
                <li>\(z_1 > 0\): \( \phi(z_1) > 0.5 \), indicating a higher probability of Class 0.</li>
                <li>\(z_1 < 0\): \( \phi(z_1) < 0.5 \), suggesting a greater likelihood of Class 1.</li>
            </ul>
        </li>
    </ul>
    <h3>Plot (b): Contour \(\phi(z_2)\)</h3>
    <ul>
        <li><strong>Class 0:</strong> Data points with \(z_2 < 0\), falling below the plane.</li>
        <li><strong>Class 1:</strong> Data points with \(z_2 > 0\), positioned above the plane.</li>
        <li><strong>Probabilities:</strong>
            <ul>
                <li>\(z_2 > 0\): \( \phi(z_2) > 0.5 \), indicating a higher probability of Class 1.</li>
                <li>\(z_2 < 0\): \( \phi(z_2) < 0.5 \), suggesting a greater likelihood of Class 0.</li>
            </ul>
        </li>
    </ul>
    <h3>Key Concepts</h3>
    <ul>
        <li><strong>Multiple Decision Boundaries:</strong> The use of two contour planes with different conditions demonstrates the ability to create complex decision boundaries for classification.</li>
        <li><strong>Softmax Function for Multinomial Distributions:</strong> Specifically designed for multi-class classification, ensuring probabilities sum to 1.</li>
        <li><strong>Probabilistic Interpretation:</strong> Provides a measure of confidence in class predictions.</li>
    </ul>
    <h3>Summary</h3>
    <ul>
        <li>The image showcases the combination of multiple decision boundaries and the softmax function for multi-class classification.</li>
        <li>This approach is essential for handling scenarios with more than two distinct classes.</li>
    </ul>
    <p>The plot gives the output, Now we need to compare both output, if the data is \(y>x\), we observe, \( \phi(z_1) > 0.5 \) and \( \phi(z_2) < 0.5 \), hence the prediction would be Class 0 and vice versa. This has been quite fun, however, more complex await.</p>
    </section>
    <section>
    <h2>Nonlinear classification</h2>
    <p>Dive into the fascinating world of nonlinear classification! We'll explore its nuances using a straightforward one-dimensional dataset. Observe how intricate decision boundaries, coupled with activation functions, effectively segregate distinct classes within this seemingly simple space.</p>
    <h3>Key Elements of the Image and Setup</h3>
    <ul>
        <li><strong>One-Dimensional Data:</strong> Data points are plotted along a single horizontal axis; showcased by the data distribution depicted in Fig. <a href="#fig:one_dsimple">1</a>.</li>
        <li><strong>Two Distinct Classes:</strong> Data is divided into Class 0 (blue points) and Class 1 (red points).</li>
    </ul>
    <figure id="fig:one_dsimple">
        <img src="one_dsimple.png" alt="Simple One Dimensional Problem.">
        <figcaption>Simple One Dimensional Problem.</figcaption>
    </figure>
    <ul>
        <li><strong>Initial Linear Separation:</strong> Dashed lines at \(x = -0.5\) and \(x = 0.5\) suggest a potential linear boundary but prove inadequate for this distribution.</li>
        <li><strong>Linear Separation:</strong> It is evident from the dataset we cannot create a linear boundary which could separate the dataset.</li>
    </ul>
    <p>To solve this problem, other than taking some complex activation for nonlinearity, we take <strong>\(x^{2}\)</strong> as an activation. Our choice is to make the understanding and intuition much easier.</p>
    \[g(x) = x^{2}\]
    <h3>Network Architecture</h3>
    <p>There is only one input parameter \(x\).</p>
    <p><strong>First Layer:</strong> Two Neurons,</p>
    \[z_{11} = a_{1}x+b_{1}\]
    \[z_{12} = a_{2}x+b_{2}\]
    <p><strong>Second Layer:</strong> Two Neurons:</p>
    <p>After using the activation function:</p>
    \[z_{1}  = d_{1}g(z_{11}) + e_{1}g(z_{12}) + f_{1}\]
    \[z_{2} = d_{2}g(z_{11}) + e_{2}g(z_{12}) + f_{2}\]
    <p>For a condition that at \(x = 0.5\), and \(x = -0.5\), the boundaries are created, we can impose a condition that \(z_{1} = 0\) and \(z_{2} = 0\) at \(x = 0.5\) and \(x = -0.5\). We also put a condition that \(z_{1} > z_{2}\) in between \(x=0.5:-0.5\) and vice versa in the other region.</p>
    <p>This gives us two solutions:</p>
    <div class="columns">
        <div class="column">
            <p><strong>First Solution</strong></p>
            \[
            \begin{align*}
            e_1 &= \frac{-a_1^2 d_1-4. f_1}{a_2^2} \\
            e_2 &= \frac{-a_1^2 d_2-4. f_2}{a_2^2} \\
            b_1  &= 0 \\
            b_2  &= 0
            \end{align*}
            \]
        </div>
        <div class="column">
            <p><strong>Second Solution</strong></p>
            \[
            \begin{align*}
            a_1 &= 0, \quad b_{1}=0 \\
            b_2 &= 0 \\
            e_1 &= -\frac{4. f_1}{a_2^2} \\
            e_2 &= -\frac{4. f_2}{a_2^2}
            \end{align*}
            \]
        </div>
    </div>
    <p><strong>First Solution</strong></p>
    <p>With \(a_{1} = 4\), \(a_2 = -4\), \(d_1 = 4\), \(d_2 = -4\), \(f_1 = -8\) and \(f_2=8\), we can calculate \(e_1=-2\) and \(e_2=2\) for first solution. This provides the <strong>Nonlinear Contour Planes</strong>, with two parabolic curves as:</p>
    \[
    \begin{align*}
    z_1 &= -8 + 32x^2 \\
    z_2 &= 8 - 32x^2
    \end{align*}
    \]
    <p>which, act as decision boundaries.</p>
    <p><strong>Second Solution</strong></p>
    <p>We observe, one important point here, With the second solution, only <strong>one neuron</strong> is required in the first hidden layer \(a_{1}=0\) and \(b_{1}=0\).</p>
    <p>This would imply the Eq. \eqref{eq:One_D_Simple} would transform as:</p>
    \[
    \begin{align*}
    z_{1} &= e_{1}(a_{2}x)^{2} + f_{1} \\
    z_{2} &= e_{2}(a_{2}x)^{2} + f_{2}
    \end{align*}
    \]
    <p>with appropriate choice \(a_{2}=0\), \(f_{1}=-f_{2}=8\); we get the expression:</p>
    \[
    \begin{align*}
    z_1 &= -8 + 32x^2 \\
    z_2 &= 8 - 32x^2
    \end{align*}
    \]
    <p>Incorporating the softmax function over \(z_1\) and \(z_2\), we get the distribution of the \(\phi(z_1)\) and \(\phi(z_2)\) for two classes shown in Fig. <a href="#fig:One_dimensional_problem_square">2</a>.</p>
    <figure id="fig:One_dimensional_problem_square">
        <img src="One_dimensional_problem_square.png" alt="(a) The plot depicts the Eqn. \(z_{1}\) and \(z_{2}\) with respect to input x. (b) The Plot depicts the distribution of probabilities with respect to \(z_{1}\) and \(z_{2}\).">
        <figcaption>(a) The plot depicts the Eqn. \(z_{1}\) and \(z_{2}\) with respect to input x. (b) The Plot depicts the distribution of probabilities with respect to \(z_{1}\) and \(z_{2}\).</figcaption>
    </figure>
    <p><strong>Class Assignment Rules</strong></p>
    <p><strong>Class 1:</strong> Points belong to Class 1 if \(z_1 < 0\) AND \(z_2 > 0\).</p>
    <p><strong>Class 0:</strong> Points belong to Class 0 if \(z_1 > 0\) OR \(z_2 < 0\) (or both).</p>
    <p><strong>Visual Interpretation of Fig. <a href="#fig:One_dimensional_problem_square">2</a>(a)</strong>:</p>
    <ul>
        <li>Points above \(z_1\) belong to Class 0.</li>
        <li>Points below \(z_2\) belong to Class 0.</li>
        <li>Points between the curves belong to Class 1.</li>
    </ul>
    <p><strong>Visual Interpretation of Fig. <a href="#fig:One_dimensional_problem_square">2</a>(b)</strong>:</p>
    <ul>
        <li>For point \(x=-0.54\), probability of class 0 \(\phi(z_{1})=0.9\) and probability of class 1 \(\phi(z_{2})=0.1\).</li>
        <li>For point \(x=0.46\), probability of class 0 \(\phi(z_{1})=0.1\) and probability of class 1 \(\phi(z_{2})=0.9\)</li>
    </ul>
    <h3>Key Concepts</h3>
    <ul>
        <li><strong>Nonlinear Decision Boundaries:</strong> Highlighting the power of nonlinear functions in capturing complex class relationships.</li>
        <li><strong>Multiple Decision Boundaries:</strong> Utilizing multiple nonlinear boundaries with different conditions for nuanced classification.</li>
        <li><strong>Activation Functions:</strong> Crucial role in introducing nonlinearity for learning complex patterns.</li>
        <li><strong>Visualizing Data Relationships:</strong> Plotting data and decision boundaries aids in understanding classification logic.</li>
    </ul>
    <h3>Summary</h3>
    <p>This image illustrates nonlinear classification in a 1D space, showcasing the combination of linear layers and nonlinear activation functions. It emphasizes the efficacy of nonlinear decision boundaries in handling complex data distributions and their significance in classification tasks.</p>
</section>
<section>
<h2>Nonlinear classification overdose</h2>
    <p>Let's delve into a more intricate problemâ€”because why not add a dash of complexity to our lives? Enter the challenging landscape of nonlinear classification, showcased by the data distribution depicted in Fig. <a href="#fig:oned_complex">1</a>.</p>
    <figure id="fig:oned_complex">
        <img src="one_dcomplex.png" alt="Complex One Dimensional Problem.">
        <figcaption>Complex One Dimensional Problem.</figcaption>
    </figure>
    <p><strong>Two Distinct Classes:</strong> The data is segregated into Class 0 (blue points) and Class 1 (red points), evenly spaced. The dashed lines at \(x = 0.25\) hint at a conceivable linear boundary.</p>
    <p>This problem appears to demand complex functions with numerous monotonically increasing and decreasing segments. Could there be a solution of that nature?</p>
    \[ g(x) = \sin(x) \]
    <h3>Network Architecture</h3>
    <p>There is only one input parameter \(x\)</p>
    <p><strong>First Layer:</strong> One Neuron</p>
    \[ z_{11} = a_{1}x+b_{1} \]
    <p><strong>Second Layer:</strong> Two Neurons</p>
    <p>After using the activation function:</p>
    \[ z_{1}  = d_{1}g(z_{11}) + f_{1} \]
    \[ z_{1} = d_{1}\sin(a_{1}x+b_{1}) +f_{1} \]
    \[ z_{2}  = d_{2}g(z_{11}) + f_{2} \]
    \[ z_{2} = d_{2}\sin(a_{1}x+b_{1}) +f_{2} \]
    <figure>
        <img src="One_dimensional_problem_sin.png" alt="(a) The plot depicts the Eqn. \(z_{1}\) and \(z_{2}\) with respect to input x. (b) The Plot depicts the distribution of probabilities with respect to \(z_{1}\) and \(z_{2}\).">
        <figcaption>(a) The plot depicts the Eqn. \(z_{1}\) and \(z_{2}\) with respect to input x. (b) The Plot depicts the distribution of probabilities with respect to \(z_{1}\) and \(z_{2}\).</figcaption>
    </figure>
    <p>For a condition that at \(x = 0.25\), and \(x = -0.25\), the boundaries are created, we can impose a condition that \(z_{1} = 0\) and \(z_{2} = 0\) at \(x = 0.25\) and \(x = -0.25\). As the solution is periodic, it will take care of the rest of the boundaries. We also put a condition that \(z_{1} > z_{2}\) in between \(x=0.0:0.25\) and vice versa. As this solution is periodic, it can have infinite solutions.</p>
    <p>We get one solution as \(a_{1} = -12\), \(b_{1}=0\), \(f_{1}=f_{2}=0\), \(d_{1}=-4\) and \(d_{2} = 4\).</p>
    <p>This would give us the solution as:</p>
    \[ z_{1}  = 4\sin(12x) \]
    \[ z_{2}  = -4\sin(12x) \]
    <p>Incorporating the softmax function over \(z_1\) and \(z_2\), we get the distribution of the \(\phi(z_1)\) and \(\phi(z_2)\) for two classes Fig.\eqref{fig:One_dimensional_problem_sin}.</p>
    <p>From the above example we can easily understand this problem, how boundaries separate class 0 and class 1.</p>
    <p>We observe only one neuron is sufficient to solve this problem. It shows that, carefully articulating an activation function can solve the same problem with smaller networks. To solve the above problem we would require multiple neurons in the first layer if consider sigmoid as an activation function (Can you think of how many, try this with your intuition).</p>
    <figure>
        <img src="One_dimensional_Sin_Lossfn.png" alt="Loss function with respect to \(\alpha\) and \(\beta\), where \(\alpha = -a_{1}/12\) and \(\beta = b_{1}\)">
        <figcaption>Loss function with respect to \(\alpha\) and \(\beta\), where \(\alpha = -a_{1}/12\) and \(\beta = b_{1}\)</figcaption>
    </figure>
    <p>Does this mean we should start using \(\sin(x)\) for all the problems? Let's study the loss function of the above example Fig. \eqref{fig:One_dimensional_Sin_Lossfn}, we have plotted the loss landscape with respect to \(\alpha = -a_{1}/12\) and \(\beta = b_{1}\) for our understanding. There are multiple local minima. The correct optimization is crucial not to get stuck in local minima. Gradient-based optimization algorithms, commonly used in training neural networks, can struggle to escape these local traps and find the true global minimum, which is crucial for optimal performance. While \(\sin\) proved effective in capturing the repeating pattern of that particular dataset, it may not generalize well to other data distributions or classification tasks. Other activation functions like ReLU, Leaky ReLU, and tanh might be more suitable in different contexts.</p>
    <hr>
    <h2>Two D problem with Nonlinearity</h2>
    <p>Lets consider more complex problem with data distribution provided in fig \eqref{fig:Concern}.</p>
    <figure id="fig:Concern">
        <img src="Concen.png" alt="Two concentric circle data.">
        <figcaption>Two concentric circle data.</figcaption>
    </figure>
    <p><strong>Two Distinct Classes:</strong> Data is divided into Class 0 (blue points) and Class 1 (red points). They are equally spaced. The dashed lines as \(x^{2} + y^{2}\) = 0.25 and subsequently shows a potential boundary.</p>
    <p>To solve this problem, other than taking some complex activation for nonlinearity, we take:</p>
    \[ g(z) = z^{2} \]
    <p>as an activation. Our choice will make the understanding and intuition much easier.</p>
    <h3>Network Architecture</h3>
    <p>There are two input parameters \(x\) and \(y\).</p>
    <p><strong>First Layer:</strong> Two Neurons</p>
    \[ z_{11} = a_{1}x+b_{1}y + c_{1} \]
    \[ z_{12} = a_{2}x+b_{2}y + c_{2} \]
    <p><strong>Second Layer:</strong> Two Neurons</p>
    \[ z_{1} = d_{1}g(z_{11}) + e_{1}g(z_{12})+f_{1} \]
    \[ z_{2} = d_{2}g(z_{11}) + e_{2}g(z_{12})+f_{2} \]
    \[ z_{1} = d_{1}(a_{1}x+b_{1}y + c_{1})^{2} + e_{1}(a_{2}x+b_{2}y + c_{2})^{2}+f_{1} \]
    \[ z_{2} = d_{2}(a_{1}x+b_{1}y + c_{1})^{2} + e_{2}(a_{2}x+b_{2}y + c_{2})^{2}+f_{2} \]
    <p>Given the condition that at \(x^{2} + y^{2} = (0.5)^2\), the boundaries are delineated, we establish a criterion where \(z_{1} = 0\) and \(z_{2} = 0\). Additionally, we enforce a condition wherein \(z_{1} > z_{2}\) within the region where \(x^{2} + y^{2} < (0.5)^2\). This condition creates contours where the probability of belonging to Class 0 is greater than that of Class 1, and vice versa.</p>
    <figure>
        <img src="Concen_softmax.png" alt="(a) The first plot defines the contour \(z_{1}\) with softmax function \(\phi(z_{1})\). (b) The second plot defines the contour \(z_{2}\) with softmax function \(\phi(z_{2})\).">
        <figcaption>(a) The first plot defines the contour \(z_{1}\) with softmax function \(\phi(z_{1})\). (b) The second plot defines the contour \(z_{2}\) with softmax function \(\phi(z_{2})\).</figcaption>
    </figure>
    <h3>Interpretation of Neural Network Plots with \(x^2\) Activation</h3>
    <h3>Key Observations</h3>
    <ul>
        <li>Concentric data distribution: The plots visualize data points arranged in two concentric circles, suggesting a radial classification task.</li>
        <li>Decision boundary: The plane \(x^2 + y^2 = 0.25\) divides the plane into two regions, acting as the decision boundary for classification.</li>
        <li>Softmax values: Softmax(\(z_1\)) and softmax(\(z_2\)) exhibit an inverse relationship across the boundary, indicating they represent probabilities for opposing classes.</li>
    </ul>
</section>
<section>
 <h2>Two D problem with Nonlinearity</h2>
    <h3>Network Architecture</h3>
    <p>There are two input parameters \( x \) and \( y \).</p>
    <h4>First Layer: Two Neurons</h4>
    \[
    \begin{align*}
        z_{11} &= a_{1}x+b_{1}y + c_{1} \\
        z_{12} &= a_{2}x+b_{2}y + c_{2}
    \end{align*}
    \]
    <h4>Second Layer: Two Neurons</h4>
    \[
    \begin{align*}
        z_{1} &= d_{1}g(z_{11}) + e_{1}g(z_{12})+f_{1} \\
        z_{2} &= d_{2}g(z_{11}) + e_{2}g(z_{12})+f_{2}
    \end{align*}
    \]
    <p>Given the activation function \( g(z) = z^{2} \), the equations become:</p>
    \[
    \begin{align*}
        z_{1} &= d_{1}(a_{1}x+b_{1}y + c_{1})^{2} + e_{1}(a_{2}x+b_{2}y + c_{2})^{2}+f_{1} \\
        z_{2} &= d_{2}(a_{1}x+b_{1}y + c_{1})^{2} + e_{2}(a_{2}x+b_{2}y + c_{2})^{2}+f_{2}
    \end{align*}
    \]
    <p>At \( x^{2} + y^{2} = (0.5)^2 \), the boundaries are delineated, where \( z_{1} = 0 \) and \( z_{2} = 0 \). Additionally, within the region where \( x^{2} + y^{2} < (0.5)^2 \), \( z_{1} > z_{2} \).</p>
    <h3>Interpretation of Neural Network Plots with \( x^2 \) Activation</h3>
    <h3>Key Observations</h3>
    <ul>
        <li>Concentric data distribution: The plots visualize data points arranged in two concentric circles, suggesting a radial classification task.</li>
        <li>Decision boundary: The plane \( x^2 + y^2 = 0.25 \) divides the plane into two regions, acting as the decision boundary for classification.</li>
        <li>Softmax values: Softmax(\( z_1 \)) and softmax(\( z_2 \)) exhibit an inverse relationship across the boundary, indicating they represent probabilities for opposing classes.</li>
    </ul>
</section>
<section>
<h2>Non-linearity with Sigmoid</h2>
    <p>We have delved in the above problem using some simple activation functions which could be easily understand and there behaviour. We will try to understand how the Sigmoid activation used in these kind of problem or more complex problems. As function is monotonically increasing, it is the linear combination of these functions which makes them universal function approximators.</p>
    <h3>Sigmoid in one D</h3>
    <p>We will first explore the one-d examples, which could be generalised to multidimensional problems. For One dimensional problem, lets pick our sigmoid from section 2:</p>
    \[
    \begin{align*}
        g(x) &= \frac{1}{1+e^{-x}} \\
        \text{For our case:} \quad z &= ax + b \\
        g(z) &= \frac{1}{1+e^{-(ax+b)}}
    \end{align*}
    \]
    <p>At \( x = -\frac{b}{a} \) the sigmoid function \( g(z)=0.5 \), which means, this point separates two regions with \( g(z)<0.5 \) and \( g(z)>0.5 \). This relation would be a crucial point in our discussions.</p>
    <h4>Subplot Fig. (a): Sigmoid Plot</h4>
    <p>The x-axis represents values of \( ax + b \), where \( a \) and \( b \) are constants that determine the shape of the curve. We can understand the how the sigmoid function behaves with respect to parameter \( a \) and \( b \).</p>
    <ul>
        <li>Larger values of \( a \) result in a steeper curve, leading to a rapid transition from 0 to 1 near the inflection point.</li>
        <li>Smaller values of \( a \) produce a gentler, smoother transition along the curve.</li>
        <li>Positive values of \( a \) shift the inflection point to the left, causing the curve to rise earlier and reach 1 faster.</li>
        <li>Negative values of \( a \) shift the inflection point to the right, resulting in a later and slower rise to 1.</li>
        <li>Positive values of \( b \) shift the curve upwards, bringing it closer to 1.</li>
        <li>Negative values of \( b \) shift the curve downwards, bringing it closer to 0.</li>
    </ul>
    <p>Imagine a horizontal line across the plot at \( y = 0.5 \) (the midpoint between 0 and 1). As you decrease the value of \( a \), this line intersects the sigmoid curve later and at a more gradual slope. In contrast, increasing \( a \) brings the intersection point closer to the inflection point and makes the transition steeper. Similarly, shifting \( b \) up or down vertically moves the entire curve without changing its shape.</p>
    <h4>Subplot Fig. (b): Two Sigmoid Combination Plot</h4>
    <p>The blue curve in plot represents the sum of two shifted sigmoid functions:</p>
    \[
    \begin{align*}
        \sigma(x+1) \text{ (purple curve)} \\
        \sigma(-x+1) \text{ (orange curve)}
    \end{align*}
    \]
    <p>Imagine each sigmoid curve starts at 0 and gradually increases to 1. Now, consider the following effects:</p>
    <ul>
        <li>Shifting by 1: Both curves shift one unit to the right (purple) and left (orange) along the x-axis. This means they start rising later compared to the standard sigmoid.</li>
        <li>Negating x-axis: The orange curve also flips horizontally, effectively mirroring it across the y-axis. This inverts its output, so it decreases from 1 to 0 instead of increasing.</li>
        <li>Summing the curves: Adding these shifted and flipped sigmoids creates the blue curve. It starts near 0, rises due to the green curve, reaches a peak, and then gradually decreases due to the orange curve.</li>
    </ul>
    <h4>Subplot Fig. (c): Two Sigmoid Combination Plot</h4>
    <p>This plot shows the sum of two sigmoid functions:</p>
    <ul>
        <li>The first sigmoid, \( \text{sigmoid}(x-1) \), is shifted one unit to the right on the \( x \)-axis.</li>
        <li>The second sigmoid, \( \text{sigmoid}(-x-1) \), is flipped horizontally (mirrored across the y-axis) and shifted one unit to the left on the \( x \)-axis.</li>
    </ul>
    <p>The resulting combined function (blue curve) starts near 0, rises due to the first sigmoid's contribution, reaches a peak, and then gradually decreases due to the counteracting effect of the flipped and shifted second sigmoid.</p>
    <h4>Subplot Fig. (d): Two Sigmoid Combination Plot</h4>
    <p>This plot shows the subtraction of two sigmoid functions:</p>
    \[
    \text{sigmoid}(x-1) - \text{sigmoid}(-x-1)
    \]
    <p>The combined function (blue curve) starts near 0, but its behavior depends on the relative strengths of the two sigmoids at different \( x \)-values.</p>
    <h4>Subplot Fig. (e): Three Sigmoid Combination Plot</h4>
    <p>This plot shows the combination of three sigmoid functions:</p>
    <ul>
        <li>sigmoid(2x + 8): This sigmoid is shifted 8 units to the left on the \( x \)-axis and stretched horizontally due to the factor of 2.</li>
        <li>sigmoid(-2x): This sigmoid is flipped horizontally (mirrored across the y-axis) and compressed horizontally due to the factor of -2.</li>
        <li>-sigmoid(-2x + 8): Similar to the second sigmoid, it's flipped and compressed, but also shifted b units to the right on the \( x \)-axis and negated (multiplied by -1).</li>
    </ul>
    <p>Now, consider how they combine:</p>
    <ul>
        <li>The first sigmoid (purple) rises due to the large positive shift, reaching high values quickly.</li>
        <li>The second sigmoid (orange) starts decreasing immediately due to the flip and compression.</li>
        <li>The third sigmoid (red), shifted positively and negated, initially decreases but eventually starts increasing as it moves further right.</li>
    </ul>
    <p>The combined function (blue) reflects the interplay of these individual contributions</p>
    <ul>
        <li>Value of b: Shifting the third sigmoid to the right (larger b) can delay its rise and influence the peak location or shape of the combined function.</li>
        <li>Scaling factors: The factors 2 and -2 in the first and second sigmoids control their steepness and the strength of their contributions.</li>
    </ul>
    <h4>Subplot Fig. (f): Four Sigmoid Combination Plot</h4>
    <p>This plot shows the combination of four sigmoid functions:</p>
    <ul>
        <li>-sigmoid(x+4): Shifted 4 units to the left along the \( x \)-axis.</li>
        <li>-sigmoid(-2x-4): Flipped, compressed horizontally due to -2, and shifted 4 units to the left.</li>
        <li>sigmoid(x-4): Shifted 4 units to the right.</li>
        <li>sigmoid(-2x+4): Flipped, compressed, and shifted 4 units to the right.</li>
    </ul>
    <p>The first and third sigmoids (purple and red) will rise due to their positive shifts. The second and fourth sigmoids (orange and blue) will decrease due to flipping and compression. The combined function (purple) reflects the interplay of these individual contributions:</p>
    <ul>
        <li>It starts near 0 due to the initial dominance of the flipped sigmoids.</li>
        <li>It might exhibit multiple peaks or valleys depending on the balance between the rising and decreasing sigmoids.</li>
        <li>Eventually, it should approach 0 as the flipped sigmoids continue to counteract the rising ones.</li>
    </ul>
    <h3>Characteristics</h3>
    <ul>
        <li>Non-monotonic behavior: Unlike a single sigmoid, this combined function is not always increasing or decreasing. It exhibits a peak point where the green curve's rise is counterbalanced by the orange curve's decline.</li>
        <li>Controllable shape: The values of \( a \) and \( b \) would influence the steepness, location of the peak, and overall shape of the combined curve.</li>
        <li>Potential applications: This type of non-linearity can be useful for modeling complex relationships in data where a simple increase or decrease wouldn't suffice. For example, it could capture phenomena with a rise and fall pattern, such as sentiment analysis or stock price fluctuations.</li>
    </ul>
    <p>Can we think how to solve the first two problems from above section in one-D using sigmoid activation. How many neurons would be required for it. Take it as an exercise...!!!!</p>
</section>
<section>
 <h3>Sigmoid in Two-D</h3>
    <p>For Two dimensional problem, lets pick our sigmoid from section 2:</p>
    \[
    \begin{align*}
        g(x) &= \frac{1}{1+e^{-x}} \\
        \text{For our case:}\quad z &= ax + by + c \\
        g(z) &= \frac{1}{1+e^{-(ax+by+c)}}
    \end{align*}
    \]
    <figure>
        <img src="Sigmoid_2D_Plots_comb1.png" alt="The Sigmoid function with its combination">
        <figcaption><em>The Sigmoid function with its combination.</em></figcaption>
        <label for="fig:Sigmoid_2D_Plots_comb1">(Fig. 1)</label>
    </figure>
    <p>at \( y = -\frac{ax}{b}-\frac{c}{b} \) the sigmoid function \( g(z)=0.5 \), which means, this plain separates two regions with \( g(z)<0.5 \) and \( g(z)>0.5 \). This relation would be a crucial point in our discussions.</p>
    <figure>
        <img src="Sigmoid_2D_Plots_comb2.png" alt="The Sigmoid function with its combination">
        <figcaption><em>The Sigmoid function with its combination.</em></figcaption>
        <label for="fig:Sigmoid_2D_Plots_comb2">(Fig. 2)</label>
    </figure>
    <h4>Subplot Fig. (a): Two Sigmoid Combination in 2-D Plot</h4>
    <ul>
        <li>Each individual sigmoid function creates a curved surface in this space, rising sharply from 0 to 1 near their activation regions.</li>
        <li>For sigmoid(x + y + 4), the activation region shifts along a diagonal plane because both x and y contribute positively.</li>
        <li>For sigmoid(-x + y + 4), the activation region is also diagonal but flipped due to the negative x term.</li>
    </ul>
    <p>The combined function (sum of both) creates a complex surface influenced by both diagonal activation regions.</p>
    <h4>Subplot Fig. (b): Three Sigmoid Combination in 2-D Plot</h4>
    <p>The three sigmoid:</p>
    <ul>
        <li>sigmoid(x + y + 4): Same as in plot (a), combining x and y with an additional positive shift.</li>
        <li>sigmoid(-x + y + 4): Also similar to plot (a), flipping the x-value and adding 4 before applying the sigmoid transformation.</li>
        <li>sigmoid(y - 4): Introduces a new component depending only on the y-value, with a negative shift of 4 before the sigmoid.</li>
    </ul>
    <p>These three are combined as:</p>
    \[
    \sigma(x,y) = 1.5 - \sigma(x + y + 4) - \sigma(-x + y + 4) + \sigma(y - 4)
    \]
    <p>We introduce a constant 1.5 to show how a contour is formed with \( \sigma < 0 \) and \( \sigma > 0 \) can be separated. The Concentric circle should remind us of the problem which we encountered earlier. While using sigmoid, it would require three neurons to fit the data.</p>
    <h4>Subplot Fig. (a,b): Three and Four Sigmoid Combination in 2-D Plot</h4>
    <p>From the above discussions, we can now understand how combination of sigmoid functions a complex pattern can be understood. We can also see, how for some problems we approached earlier, only one neurons with specific activation was suffice, however, more neurons are required by using sigmoid activation. This is because sigmoid is monotonically increasing/decreasing function, many neurons are required to generate a curve/complex pattern.</p>
</section>
</body>
</html>

