<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Peeping Inside black hole</title>
</head>
<body>
<h1>Peeping Inside black box : I</h1>
<p>The field of machine learning has come a long way in recent decades. Traditional algorithms were easier to
    understand, offering clear insights into their internal workings. However, the rise of deep neural networks, with
    their vast and complex architectures, has presented a new challenge: deciphering their hidden mechanisms. While
    some techniques exist, akin to local minima analysis in traditional ML, a deeper understanding of deep networks
    remains tantalisingly out of reach. This lack of clarity casts a shadow on our comprehension of various other
    algorithms as well.
    Though I won’t delve into the intricate details of deep learning algorithms, I aim to shed some light on their
    internal workings with shallow networks, drawing parallels to my past experience in theoretical physics. In my
    interactions with fellow physicists, grappling with the meaning and significance of complex mathematical expressions
    with physical relevance was a recurring challenge. A similar question arose regarding the nature of neurons in
    artificial neural networks: can we truly understand and explain these fundamental building blocks? This very
    curiosity ignites my desire to explore this realm further.
    The magic of deep learning lies in its ability to bypass the tedious, manual process of feature engineering.
    Instead, by employing cleverly designed architectures and optimisation techniques, the network itself learns to
    identify and extract relevant features from data.
    In the initial section, we’ll delve into regression problems, followed by an expansion into classification problems
    in the subsequent section. Then, we’ll delve into insights on the well-known Sigmoid activation function, examining
    both one-dimensional and two-dimensional scenarios. We’ll also explore feature representation from these models.
    Subsequently, we’ll present a simple real-life problem from NLP, marking the beginning of a more intricate journey.
    In the next work, we’ll delve into more complex deeper networks and the renowned transformer architecture.</p>
<p>Click the link below to open the PDF:</p>

<a href="Peeping_Inside_Black_box.pdf" target="_blank">Open PDF</a>

</body>
</html>
