<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Merged Document</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Peeping Inside Black box : ML to DL</h1>
    <section>
        <h2>Introduction</h2>
        <p>The field of machine learning has come a long way in recent decades. Traditional algorithms were easier to understand, offering clear insights into their internal workings. However, the rise of deep neural networks, with their vast and complex architectures, has presented a new challenge: deciphering their hidden mechanisms. While some techniques exist, akin to local minima analysis in traditional ML, a deeper understanding of deep networks remains tantalizingly out of reach. This lack of clarity casts a shadow on our comprehension of various other algorithms as well.</p>

          <p>Though I won't delve into the intricate details of deep learning algorithms, I aim to shed some light on their internal workings with shallow networks, drawing parallels to my past experience in theoretical physics. In my interactions with fellow physicists, grappling with the meaning and significance of complex mathematical expressions with physical relevance was a recurring challenge. A similar question arose regarding the nature of neurons in artificial neural networks: can we truly understand and explain these fundamental building blocks? This very curiosity ignites my desire to explore this realm further.</p>

        <p>The magic of deep learning lies in its ability to bypass the tedious, manual process of feature engineering. Instead, by employing cleverly designed architectures and optimization techniques, the network itself learns to identify and extract relevant features from data.</p>

        <p>In the initial section, we'll delve into regression problems, followed by an expansion into classification problems in the subsequent section. Then, we'll delve into insights on the well-known Sigmoid activation function, examining both one-dimensional and two-dimensional scenarios. We'll also explore feature representation from these models. Subsequently, we'll present a simple real-life problem from NLP, marking the beginning of a more intricate journey. In the next work, we'll delve into more complex deeper networks and the renowned transformer architecture.</p>

    </section>

    <section>
        <h2>Machine Learning Approach</h2>
        <p>To illustrate the concept of Machine learning, let's dive into a practical example. We'll take a deep dive into a specific dataset.</p>

        <table>
            <caption>Input-Output Table</caption>
            <thead>
                <tr>
                    <th>Input</th>
                    <th>Output</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>2</td>
                    <td>4.04</td>
                </tr>
                <tr>
                    <td>2.5</td>
                    <td>6.28</td>
                </tr>
                <!-- More data rows -->
            </tbody>
        </table>

        <p>We have a collection of data points scattered on a graph, with each point representing a relationship between two variables. Your goal is to find a line or curve that best captures the overall trend of these points. This line or curve is your model, and the process of finding it is called "fitting" the model to the data.</p>

        <p><strong>This dataset holds immense potential for uncovering hidden patterns and insights.</strong> To unlock its secrets, we'll embark on a journey of discovery using the powerful tools of machine learning. Let's take it step-by-step:</p>

        <figure>
            <img src="Example_Plot.png" alt="Example Plot">
            <figcaption>The Comparison between Actual and Model.</figcaption>
        </figure>

        <p><strong>Step 1: Building a Simple Model</strong></p>

        <p>Imagine we have a machine that takes numbers as input (\( x \)) and outputs other numbers (\( X \)). To understand the relationship between \( x \) and \( X \), we can start with a simple model:</p>

        <p><strong>Model:</strong> \( Y = aX \)</p>

        <p>This equation says that \( Y \) is simply a scaled version of \( X \), determined by the coefficient "\( a \)". It's like stretching or shrinking \( X \) according to the value of \( a \).</p>

        <p><strong>Step 2: Testing the Model's Fit</strong></p>

        <p>Can this simple model handle all the data? We plug in different values of \( X \) and compare the predicted \( Y \) values (using our model) to the actual \( Y \) values in the dataset. This comparison reveals:</p>

        <p><strong>Limitations:</strong> If the data exhibits complex patterns beyond just scaling, the \( Y \) predictions from our simple model will likely deviate significantly from the actual values.</p>

        <p><strong>Step 3: Introducing Complexity</strong></p>

        <p>To capture these intricate patterns, we can add more sophistication to our model. One way is to introduce a quadratic term:</p>

        <p><strong>Model:</strong> \( Y = aX + bX^2 \)</p>

        <p>This equation adds a "bendiness" factor to the model, controlled by the coefficient "\( b \)". Now, by adjusting both \( a \) (slope) and \( b \) (bendiness), we can potentially achieve a much better fit to the data. It appears that \( a \approx 0 \) and \( b\approx1 \) remarkably align with our dataset Fig.1.</p>

        <p>However, it's lamentable that we had to infer on our own which polynomial terms would best suit our model's fit. Choosing the right terms and complexity remains an exciting challenge in machine learning. Through continued exploration and analysis, we can unlock the secrets hidden within this dataset and gain valuable insights from its hidden patterns.</p>
    </section>
    <section>
        <h2>Deep Learning Approach</h2>

        <p>Let's explore this problem using a neural network. Our objective? Discovering the simplest network suitable for our dataset. Any specific normalization conditions or other considerations?</p>

        <p>Let's start with neurons in a basic architecture:</p>

        <p>\[ z = ax + b \]</p>

        <p>These neurons follow a linear path. No matter how many layers I add, the solution remains linear. But let's set aside the technical terms for a moment and put it to the test.</p>

        <p>Imagine we have just two neurons, and we combine them:</p>

        \[
        \begin{align*}
        z_{1} &= a_{1}x + b_{1} \\
        z_{2} &= a_{2}x + b_{2} \\
        \ldots \\
        z_{3} &= z_{1} + z_{2} \\
        z_{3} &= (a_{1}+a_{2})x + (b_{1}+b_{2})
        \end{align*}
        \]

        <p>Whoops! It seems the result remains linear. Even when we bring these neurons together, the path they follow stays just as straight. Nonlinearity? Not happening here!</p>
    </section>
    
    <section>
        <h2>Sigmoid: Non Linearity</h2>
        <h3>What next, we need non-linearity in our model. Let's consider the Sigmoid function and study it a bit.</h3>
<p>For our case: \( z = y + b \) ; where \( y = ax \)</p>
    <p>\( g(z) = \frac{1}{1+e^{-(y+b)}} \)</p>
    <h3>This looks complex... Can we really make sense out of it...!!!</h3>
    <div class="md-frame">
        <h4>Taylor Series Expansion</h4>
        <p>The Taylor series expansion of a function \( f(x) \) around a point \( a \) is given by:</p>
        <p>\[ f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \dotsb \]</p>
        <p>In this expansion:</p>
        <ul>
            <li>\( f(a) \) represents the value of the function at \( x=a \).</li>
            <li>\( f'(a) \) represents the first derivative of \( f(x) \) evaluated at \( x=a \).</li>
            <li>\( f''(a) \) represents the second derivative of \( f(x) \) evaluated at \( x=a \).</li>
            <li>Similarly, \( f'''(a) \) represents the third derivative, and so on.</li>
        </ul>
        <p>The \( n \)th term in the expansion involves the \( n \)th derivative of \( f(x) \) evaluated at \( x=a \) divided by \( n! \) times \( (x-a)^n \).</p>
        <p>The Taylor series expansion is a way to represent a function as an infinite sum of terms involving its derivatives evaluated at a specific point \( a \). This expansion is particularly useful in calculus and mathematical analysis for approximating functions around a given point.</p>
    </div>
    <div class="md-frame">
        <h4>Approximate Sigmoid Function</h4>
        <p>For the Sigmoid function around \( y \approx 0 \) or \( |y| < 1 \):</p>
        <p>\[ g(z) \approx g(b) + yg'(b) + \frac{y^{2}}{2!}g''(b) + \frac{y^{3}}{3!}g'''(b) + \frac{y^{4}}{4!}g''''(b) + \dots \]</p>
        <p>where,</p>
        <p>\( g'(b), g''(b), g'''(b) \) and \( g''''(b) \) are the first, second, third, and fourth derivatives of the sigmoid function \( g(x) \) evaluated at \( b \).</p>
    </div>
    <h3>Evaluation of Approximation:</h3>
    <ul>
        <li><strong>Suitable Approximation (\( \lvert y \rvert < 1 \)): </strong>For \( \lvert y \rvert < 1 \), the approximation closely matches the sigmoid function for any value of \( b \).</li>
        <li><strong>Inadequate Approximation (\( |y|>1 \)): </strong>When \( |y|>1 \), the approximation doesn't hold for all values of \( b \).</li>
    </ul>
    <div class="figure">
        <img src="image_b_good.png" alt="Good sigmoid approximation">
        <p class="caption">(a) The approximated sigmoid function works pretty good for \( |y|<1 \), for all the values of \( b \).</p>
    </div>
    <div class="figure">
        <img src="image_b_bad.png" alt="Bad sigmoid approximation">
        <p class="caption">(b) The Sigmoid approximation for \( |y|>1 \), is only satisfied for range of \( b \) values where \( |y+b| < 1 \).</p>
    </div>
    <h3>Key Implications:</h3>
    <ul>
        <li><strong>Model Conditioning:</strong> To guarantee model accuracy, we prioritize situations where \( \lvert y \rvert < 1 \), effectively imposing a constraint on the model bias.</li>
        <li><strong>Data Normalization:</strong> This finding emphasizes the crucial role of data normalization or rescaling within a range satisfying \( \lvert ax \rvert < 1 \).</li>
        <li><strong>Deep Learning Connection:</strong> This principle aligns with established deep learning practices, such as batch normalization and data scaling.</li>
    </ul>
    <h3>Conclusion:</h3>
    <p>The analysis underscores the vital importance of data normalization for the validity of the sigmoid function approximation. This ensures model accuracy and aligns with standard practices in deep learning.</p>
    <div class="figure">
        <img src="image_b_derivative.png" alt="Derivatives of Sigmoid Function">
        <p class="caption">The derivatives of Sigmoid Function.</p>
    </div>
    <p>While a formal proof falls outside the scope of our current discussion, visual examination of the derivative of sigmoid functions, as depicted in Figure 1, reveals a salient characteristic: their values consistently remain bounded below unity.</p>
    <p>The same goes for sigmoid most closest ally \( \tanh \) activation function. We have shown that in Appendix A.</p>
    </section>

    <section>
    <h2>The Quest Begins: Embracing Complexity to Capture \( X = x^2 \)</h2>
    <p>Our journey delves into the intriguing realm of neural networks, where we embark on a mission to conquer the challenge of approximating a non-linear relationship: \( X \approx x^2 \). Through this exploration, we unveil the limitations and hidden potential of different network architectures, paving the way for a deeper understanding of their expressive power.</p>
    <h3>Single Neuron's Stumble: The Linearity Barrier</h3>
    <p><strong>Initial Exploration:</strong> We commence our quest with a single neuron armed with linear activation:</p>
    \[ z = ax + b \]
    \[ X = g(z) \]
    <div class="figure">
        <img src="one_neuron.png" alt="The One neuron architecture of DL">
        <p class="caption">(a) The One neuron architecture of DL.</p>
    </div>
    <ul>
        <li><strong>Taylor Series Unveiling:</strong> To illuminate the neuron's behavior, we wield the tool of a Taylor series expansion around the bias \( b \).</li>
    </ul>
    \[ X \approx g(b) + (ax)g'(b) + \frac{(ax)^2}{2!}g''(b) + \dots \]
    <ul>
        <li><strong>Focusing on the Leading Terms:</strong> Scrutinizing the dominant terms, we encounter a crucial insight:</li>
    </ul>
    \[ X \approx g(b) + (ax)g'(b) + \frac{(ax)^2}{2!}g''(b) \]
    <p><strong>Hitting the Wall:</strong> Despite the non-linear activation function \( g \), the single neuron's inherent linearity shackles its ability to fully capture the quadratic curvature of \( X \approx x^2 \). This limitation impedes our network from accurately modeling the desired relationship.</p>
    <h3>Enhancing Expressive Capacity with Two Neurons</h3>
    <div class="figure">
        <img src="two_neuron.png" alt="The Two neuron architecture of DL">
        <p class="caption">(b) The Two neuron architecture of DL.</p>
    </div>
    <p>To transcend the limitations posed by a solitary neuron, we advance towards a more potent architecture; dual-neuron network:</p>
    \[
    \begin{aligned}
    z_1 &= a_{11}x + b_{11} \\
    z_2 &= a_{12}x + b_{12} \\
    X &= a_{21}g(z_1) + a_{22}g(z_2) + b_2
    \end{aligned}
    \]
    <p>Focusing on leading terms:</p>
    \[
    \begin{aligned}
    X &\approx  b_{2} + a_{21} g(b_{11}) + a_{22} g(b_{12}) + x (a_{11} a_{21} g'(b_{11}) + a_{12} a_{22} g'(b_{12})) \\
    &+ \frac{x^2}{2} (a_{11}^2 a_{21} g''(b_{11}) + a_{12}^2 a_{22} g''(b_{12})) + \dots
    \end{aligned}
    \]
    <p>Exploring the Taylor series for both neurons and focusing on the leading terms, we derive the weight and bias conditions enabling \( X \approx x^2 \):</p>
    \[
    \begin{aligned}
    a_{21} &= \frac{2g'(b_{12})}{a_{11}^2 g'(b_{12}) g''(b_{11}) - a_{11} a_{12} g'(b_{11}) g''(b_{12})} \\
    a_{22} &= -\frac{2g'(b_{11})}{a_{11} a_{12} g'(b_{12}) g''(b_{11}) - a_{12}^2 g'(b_{11}) g''(b_{12})} \\
    b_2 &= \frac{2(a_{11}g(b_{12})g'(b_{11})- a_{12}g(b_{11})g'(b_{12}))}{a_{11}^2 g'(b_{12}) g''(b_{11}) - a_{11} a_{12} g'(b_{11}) g''(b_{12})}
    \end{aligned}
    \]
    <p>Simplifying further, we refine these conditions by imposing constraints like \( b_{11} = b_{12} \) and \( a_{11} = -a_{12} \):</p>
    \[
    \begin{aligned}
    a_{21} = a_{22} &= \frac{1}{a_{12}^2 g''(b_{12})} \\
    b_2 &= -\frac{2g(b_{12})}{a_{12}^2 g''(b_{12})}
    \end{aligned}
    \]
    <p>Ensuring \( |a_{11}x|<1 \) guarantees the convergence of the Taylor series expansion, adding a layer of stability to the model.</p>
    <p>Understanding the intuition behind these equations is pivotal. The combination of weighted non-linear activations from both neurons enriches the input representation. This capability enables the network to capture the quadratic curvature of \( X \approx x^2 \) that a single neuron cannot.</p>
    <p>Consider plotting each neuron's activation function alongside the desired quadratic function. This visualization illustrates how the dual-neuron network can flex its combined output to more closely approximate the quadratic curve.</p>
    <p>This two-neuron framework signifies a notable advancement in expressive potential compared to a solitary neuron. By thoughtfully configuring weights and biases, we harness the synergy of multiple neurons to tackle intricate non-linear relationships.</p>
    <h3>Evaluation of Model</h3>
    <figure>
        <pre><code>class PolynomialRegression(nn.Module):
    def __init__(self):
        super(PolynomialRegression, self).__init__()
        self.linear1 = nn.Linear(1, 2)
        self.m = nn.Sigmoid()
        self.linear2 = nn.Linear(2, 1)

    def forward(self, x):
        x = self.linear1(x)
        x = self.m(x)
        x = self.linear2(x)
        return x

linear1.weight = [[-0.01],[0.01]]
linear2.weight = [[-3.73344*pow(10,5),-3.73344*pow(10,5)]]
linear1.bias = [3.5,3.5]
linear2.bias = [7.248*pow(10,5)]</code></pre>
        <figcaption>Pytorch Module For Square Model</figcaption>
    </figure>
    <figure>
        <img src="Square.png" alt="The Comparison between Actual and Model">
        <figcaption>(a) The Comparison between Actual and Model.</figcaption>
    </figure>
    <p>Let's assess the performance and validity of this model. This code snippet defines a <code>Polynomial Regression</code> class using PyTorch's neural network module. It involves two linear layers along with a sigmoid activation function. The weights and biases are preset for each layer, with explicit configurations provided for better understanding and evaluation.</p>
    <p>For values \( a_{12} = - a_{11} = -0.01 \) and \( b_{12} = 3.5 \), we would get \( a_{21} = a_{22} = -3.73344 \times 10^{5} \) and \( b_2 = 7.248 \times 10^{5} \). This would provide a suitable fit as Shown in Fig. (a), where actual and model prediction coincide with minimum error.</p>
    <p>This setup aims to approximate a polynomial relationship within the data. We'll proceed to test this model.</p>
    <p>While the current approach holds promise, could we potentially push the envelope further? Perhaps incorporating higher-order terms, such as cubic terms, into the model might yield even more accurate results.</p>
</section>
<section>
 <h2>Delving Deeper: Approximating \( X \approx x^3 \)</h2>
    <p><strong>Unveiling the Power of Two</strong></p>
    <p>Can a mere pair of neurons conquer the cubic realm? Let's embark on this exploration, armed with mathematical insights and computational prowess.</p>
    <p><strong>Taylor Series Ascendancy</strong></p>
    <p>We commence by invoking the revered Taylor series to unveil the hidden facets of our two-neuron architecture:</p>
    <h4>Equations Unveiled</h4>
    \[
    \begin{align*}
    z_1 &= a_{11}x + b_{11} \\
    z_2 &= a_{12}x + b_{12} \\
    X &= a_{21}g(z_1) + a_{22}g(z_2) + b_2
    \end{align*}
    \]
    <p>Meticulously expanding these equations, we isolate the leading terms, revealing a pathway towards cubic approximation:</p>
    \[
    \begin{align*}
    X &\approx b_2 + a_{21}g(b_{11}) + a_{22}g(b_{12}) + x(a_{11}a_{21}g'(b_{11}) + a_{12}a_{22}g'(b_{12})) \\
    &+ \frac{x^2}{2}(a_{11}^2a_{21}g''(b_{11}) + a_{12}^2a_{22}g''(b_{12})) + \frac{x^3}{6}(a_{11}^3a_{21}g'''(b_{11}) + a_{12}^3a_{22}g'''(b_{12})) + \dots
    \end{align*}
    \]
    <p><strong>Condition for Cubic Conquest</strong></p>
    <p>To achieve the coveted \( X\approx x^{3} \), we meticulously derive a set of constraints upon the weights and biases:</p>
    <h4>Equations Imposed</h4>
    \[
    \begin{align*}
    a_{12} &= \frac{a_{11}g''(b_{11})g'(b_{12})}{g'(b_{11})g''(b_{12})} \\
    a_{21} &= \frac{6g'(b_{11})g''(b_{12})^2}{a_{11}^3(g'(b_{11})g''(b_{12})^2g'''(b_{11})-g'(b_{12})g''(b_{11})^2g'''(b_{12}))} \\
    a_{22} &= -\frac{6g'(b_{11})^3g''(b_{12})^3}{a_{11}^3g'(b_{12})^2(g'(b_{11})g''(b_{11})g''(b_{12})^2g'''(b_{11})-1g'(b_{12})g''(b_{11})^3g'''(b_{12}))} \\
    b_{21} &= -\frac{6g'(b_{11})g''(b_{12})^2(g(b_{11})g'(b_{12})^2g''(b_{11})-g(b_{12})g'(b_{11})^2g''(b_{12}))}{a_{11}^3g'(b_{12})^2(g'(b_{11})g''(b_{11})g''(b_{12})^2g'''(b_{11})-g'(b_{12})g''(b_{11})^3g'''(b_{12})}
    \end{align*}
    \]
    <p><strong>Numerical Manifestation</strong></p>
    <p>We translate these theoretical insights into the realm of computation, employing Python to craft a tangible model:</p>
    <div class="figure">
        <img src="Cube.png" alt="The Comparison between Actual and Model">
        <p class="caption">(a) The Comparison between Actual and Model.</p>
    </div>
    <pre class="md-frame">
        <code class="language-python">
class PolynomialRegression(nn.Module):
    def __init__(self):
        super(PolynomialRegression, self).__init__()
        self.linear1 = nn.Linear(1, 2) # Weight matrix definition: Size - Number of features x Number of targets
        self.m = nn.Sigmoid()
        self.linear2 = nn.Linear(2, 1) # Weight matrix definition: Size - Number of features x Number of targets

    def forward(self, x):
        x = self.linear1(x)
        x = self.m(x)
        x = self.linear2(x)
        return x
data1 = [[0.01],[-0.0490245]]
data2 = [[2.21685*pow(10,6),4.2613*pow(10,5)]]
bias1 = [0.5,-0.1]
bias2 = [-1.58232*pow(10,6)]
        </code>
    </pre>
    <p>Let's evaluate the efficacy and reliability of this model. The following code snippet establishes a <code>PolynomialRegression</code> class utilizing PyTorch's neural network module. It comprises two linear layers along with a sigmoid activation function. Explicitly predefined weights and biases for each layer are included to enhance comprehension and facilitate assessment.</p>
    <p>For values \( a_{11}= 0.01 \), \( b_{11} = 0.5 \) and \( b_{12} = -0.1 \), we would get \( a_{12}= -0.0490245 \), \( a_{21} = 2.21685 \times 10^{6} \), \( a_{22} = 4.2613 \times 10^{5} \) and \( b_{21}=-1.58232\times 10^{6} \). This would provide a suitable fit as Shown in Fig. (a), where actual and model prediction coincide with minimum error.</p>
    <p>This configuration endeavors to mimic a polynomial association within the dataset. Our next step involves testing this model's performance and meticulously analyzing its accuracy in capturing the inherent data patterns.</p>
</section>
<section>
<h2>Unveiling the Art of Multiplication with Neurons: A Symphony of Hidden Layers</h2>
    <p><strong>The Elusive Nature of Multiplication in Single Neurons</strong></p>
    <p>Individual neurons are great at understanding simple connections between things, but they struggle when it comes to directly understanding something like multiplying two things together, like \( xy \).</p>
    <div class="figure">
        <img src="four_neurons.png" alt="The Four neuron architecture of DL">
        <p class="caption">Fig. The Four neuron architecture of DL.</p>
    </div>
    <p>How can we solve this with two inputs, \( x \) and \( y \)? If we use just one neuron:</p>
    \[
    \begin{align*}
    z &= a_{11}x+a_{12}y+b_{11} \\
    g(z) &\approx f(b_{11}) + a_{11} x g^{'}(b_{11})+a_{12} y g^{'}(b_{11})+\frac{1}{2} a_{11}^2 x^2 g^{''}(b_{11})+\frac{1}{2} a_{12}^2 y^2 g^{''}(b_{11}) +a_{12} a_{11} \textcolor{red}{x y}
    g^{''}(b_{11})+ \dots
    \end{align*}
    \]
    <p>We're looking for a condition where the term that involves \( xy \) is the most important. This is quite a challenging problem, but there might be a clever way to simplify it.</p>
    <p>This kind of math is trickier for a single neuron because it's not just a straightforward relationship. To tackle this, we need a smarter setup: a team of neurons that can work together to handle these complex tasks.</p>
    <p><strong>Constructing a Concerto of Neurons</strong></p>
    <p>The architectural masterpiece we unveil unfolds in two acts as Shown in Fig. The Four neuron architecture of DL:</p>
    <p><strong>Act I: The First Hidden Layer - A Quartet of Emphasis</strong></p>
    <p>Visualize four specialized neurons, each finely tuned to magnify the representation of \( xy \) in their activities. We consider 4 neurons with \( x \) and \( y \) as input. The first hidden layer would be:</p>
    \[
    \begin{align*}
    z_{11} &= a (x+y)+b \\
    g(z_{11}) &= \frac{1}{2} a^2 x^2 g^{''}(b)+a^2 \textcolor{red}{x y} g^{''}(b)+\frac{1}{2} a^2 y^2 g^{''}(b)+a x g^{'}(b)+a y g^{'}(b)+g(b) \\
    z_{12} &= a (-x-y)+b \\
    g(z_{12}) &= \frac{1}{2} a^2 x^2 g^{''}(b)+a^2 \textcolor{red}{x y} g^{''}(b)+\frac{1}{2} a^2 y^2 g^{''}(b)-a x g^{'}(b)-a y g^{'}(b)+g(b) \\
    z_{13} &= a (-x+y)+b \\
    g(z_{13}) &= \frac{1}{2} a^2 x^2 g^{''}(b)-a^2 \textcolor{red}{x y} g^{''}(b)+\frac{1}{2} a^2 y^2 g^{''}(b)-a x g^{'}(b)+a y g^{'}(b)+g(b) \\
    z_{14} &= a (x-y)+b \\
    g(z_{14}) &= \frac{1}{2} a^2 x^2 g^{''}(b)-a^2 \textcolor{red}{x y} g^{''}(b)+\frac{1}{2} a^2 y^2 g^{''}(b)+a x g^{'}(b)-a y g^{'}(b)+g(b) \\
    \end{align*}
    \]
    <p>These neurons combine different ways of adding and subtracting \( x \) and \( y \), boosted by a carefully chosen number 'a'. This special number helps make the desired product stronger in each neuron's activity.</p>
    <p><strong>Act II: The Unifying Maestro - Harmonising the Ensemble</strong></p>
    <p>A second neuron emerges, playing the role of the maestro, orchestrating the outputs of the first layer:</p>
    \[
    \begin{align*}
    X &= cg(z_{11}) + cg(z_{12}) - cg(z_{13}) - cg(z_{14}) \\
    &= 4a^2 c xy g^{''}(b)
    \end{align*}
    \]
    <p>Here, \( g(z) \) represents the activation function, guiding the flow of information within the network. \( c \) acts as a critical conductor, carefully tuning the contributions of each neuron. By setting \( c = \frac{1}{4a^2 g''(b)} \), where \( g''(z) \) denotes the second derivative of the activation function, we achieve a remarkable transformation: \( X \approx xy \), harmoniously blending the outputs of the first layer through the chosen activation function and meticulously selected parameters.</p>
    <h3>From Theory to Code: Translating the Concerto into Python</h3>
    <div class="figure">
        <img src="Multiply.png" alt="The Comparison between Actual and Model">
        <p class="caption">(b) The Comparison between Actual and Model.</p>
    </div>
    <p>This code snippet defines a <code>Multiply</code> class in PyTorch, incorporating two linear layers to construct a neural network. The first layer has four neurons to emphasize the interaction between \( x \) and \( y \), while the second layer produces the output based on this interaction. This configuration aims to capture the relationship \( xy \) within the dataset.</p>
    <p>For \( a=0.01 \), \( b-1.5 \), we get \( c = -2.6390\times10^{4} \), would provide a suitable fit as Shown in Fig. (b), where actual and model prediction coincide with minimum error.</p>
    <p>Neural networks can approximate non-linear functions like multiplication. Strategically designed neuron ensembles can isolate specific terms and relationships. Understanding these techniques empowers us to tackle increasingly intricate problems in neural computation.</p>
</section>
</body>
</html>

